{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":374754,"status":"ok","timestamp":1703393387149,"user":{"displayName":"PUJARI AISHWARYA","userId":"12213497540452541192"},"user_tz":-330},"id":"02tLINounVRk","outputId":"a529d5c3-0407-47b8-eb09-4a4b95ff41d2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Error processing image /content/drive/MyDrive/dataset/acne/.ipynb_checkpoints: OpenCV(4.8.0) /io/opencv/modules/imgproc/src/resize.cpp:4062: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n","\n","Shapes - TrainX: (72, 224, 224, 3) TrainY: (72, 4)\n","Shapes - TestX: (18, 224, 224, 3) TestY: (18, 4)\n","Label Encoder Classes: ['acne' 'acne_scars' 'hyperPigmentation' 'white_patches']\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d (Conv2D)             (None, 222, 222, 32)      896       \n","                                                                 \n"," max_pooling2d (MaxPooling2  (None, 111, 111, 32)      0         \n"," D)                                                              \n","                                                                 \n"," batch_normalization (Batch  (None, 111, 111, 32)      128       \n"," Normalization)                                                  \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 109, 109, 64)      18496     \n","                                                                 \n"," max_pooling2d_1 (MaxPoolin  (None, 54, 54, 64)        0         \n"," g2D)                                                            \n","                                                                 \n"," batch_normalization_1 (Bat  (None, 54, 54, 64)        256       \n"," chNormalization)                                                \n","                                                                 \n"," conv2d_2 (Conv2D)           (None, 52, 52, 128)       73856     \n","                                                                 \n"," max_pooling2d_2 (MaxPoolin  (None, 26, 26, 128)       0         \n"," g2D)                                                            \n","                                                                 \n"," batch_normalization_2 (Bat  (None, 26, 26, 128)       512       \n"," chNormalization)                                                \n","                                                                 \n"," conv2d_3 (Conv2D)           (None, 24, 24, 256)       295168    \n","                                                                 \n"," max_pooling2d_3 (MaxPoolin  (None, 12, 12, 256)       0         \n"," g2D)                                                            \n","                                                                 \n"," batch_normalization_3 (Bat  (None, 12, 12, 256)       1024      \n"," chNormalization)                                                \n","                                                                 \n"," flatten (Flatten)           (None, 36864)             0         \n","                                                                 \n"," dense (Dense)               (None, 128)               4718720   \n","                                                                 \n"," dropout (Dropout)           (None, 128)               0         \n","                                                                 \n"," dense_1 (Dense)             (None, 4)                 516       \n","                                                                 \n","=================================================================\n","Total params: 5109572 (19.49 MB)\n","Trainable params: 5108612 (19.49 MB)\n","Non-trainable params: 960 (3.75 KB)\n","_________________________________________________________________\n","Epoch 1/20\n","3/3 [==============================] - 15s 4s/step - loss: 8.7397 - accuracy: 0.2778 - val_loss: 1.3890 - val_accuracy: 0.1667 - lr: 0.0010\n","Epoch 2/20\n","3/3 [==============================] - 14s 4s/step - loss: 7.6343 - accuracy: 0.3889 - val_loss: 1.0440 - val_accuracy: 0.6667 - lr: 0.0010\n","Epoch 3/20\n","3/3 [==============================] - 14s 3s/step - loss: 7.5593 - accuracy: 0.3889 - val_loss: 1.4303 - val_accuracy: 0.3889 - lr: 0.0010\n","Epoch 4/20\n","3/3 [==============================] - 13s 4s/step - loss: 9.0508 - accuracy: 0.3611 - val_loss: 0.9525 - val_accuracy: 0.7222 - lr: 0.0010\n","Epoch 5/20\n","3/3 [==============================] - 13s 4s/step - loss: 9.1145 - accuracy: 0.4861 - val_loss: 1.6735 - val_accuracy: 0.7222 - lr: 0.0010\n","Epoch 6/20\n","3/3 [==============================] - 13s 6s/step - loss: 8.5211 - accuracy: 0.4722 - val_loss: 2.2923 - val_accuracy: 0.1667 - lr: 9.0000e-04\n","Epoch 7/20\n","3/3 [==============================] - 13s 6s/step - loss: 5.4335 - accuracy: 0.5417 - val_loss: 1.5753 - val_accuracy: 0.6111 - lr: 9.0000e-04\n","Epoch 8/20\n","3/3 [==============================] - 13s 3s/step - loss: 7.0090 - accuracy: 0.4167 - val_loss: 1.6262 - val_accuracy: 0.5556 - lr: 9.0000e-04\n","Epoch 9/20\n","3/3 [==============================] - 13s 6s/step - loss: 6.2740 - accuracy: 0.4583 - val_loss: 1.0835 - val_accuracy: 0.6111 - lr: 9.0000e-04\n","Epoch 10/20\n","3/3 [==============================] - 16s 4s/step - loss: 4.9919 - accuracy: 0.4861 - val_loss: 2.3906 - val_accuracy: 0.2222 - lr: 9.0000e-04\n","Epoch 11/20\n","3/3 [==============================] - 13s 5s/step - loss: 5.4902 - accuracy: 0.5694 - val_loss: 5.2541 - val_accuracy: 0.1667 - lr: 8.1000e-04\n","Epoch 12/20\n","3/3 [==============================] - 21s 9s/step - loss: 7.0787 - accuracy: 0.4583 - val_loss: 1.4426 - val_accuracy: 0.5556 - lr: 8.1000e-04\n","Epoch 13/20\n","3/3 [==============================] - 11s 3s/step - loss: 4.7812 - accuracy: 0.5833 - val_loss: 1.8443 - val_accuracy: 0.6667 - lr: 8.1000e-04\n","Epoch 14/20\n","3/3 [==============================] - 13s 3s/step - loss: 5.6356 - accuracy: 0.5972 - val_loss: 1.8541 - val_accuracy: 0.6667 - lr: 8.1000e-04\n","Epoch 15/20\n","3/3 [==============================] - 12s 4s/step - loss: 6.4201 - accuracy: 0.5694 - val_loss: 1.2070 - val_accuracy: 0.5000 - lr: 8.1000e-04\n","Epoch 16/20\n","3/3 [==============================] - 13s 6s/step - loss: 6.3342 - accuracy: 0.4444 - val_loss: 1.1995 - val_accuracy: 0.5000 - lr: 7.2900e-04\n","Epoch 17/20\n","3/3 [==============================] - 12s 5s/step - loss: 5.9269 - accuracy: 0.5139 - val_loss: 1.6280 - val_accuracy: 0.5000 - lr: 7.2900e-04\n","Epoch 18/20\n","3/3 [==============================] - 13s 6s/step - loss: 4.3476 - accuracy: 0.5694 - val_loss: 2.6503 - val_accuracy: 0.2778 - lr: 7.2900e-04\n","Epoch 19/20\n","3/3 [==============================] - 15s 5s/step - loss: 2.3589 - accuracy: 0.6389 - val_loss: 2.9796 - val_accuracy: 0.2778 - lr: 7.2900e-04\n","Epoch 20/20\n","3/3 [==============================] - 14s 4s/step - loss: 4.2358 - accuracy: 0.5694 - val_loss: 2.8206 - val_accuracy: 0.5000 - lr: 7.2900e-04\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"name":"stdout","output_type":"stream","text":["{'loss': [8.73965072631836, 7.634267330169678, 7.559305191040039, 9.050756454467773, 9.11449146270752, 8.521060943603516, 5.433544635772705, 7.009028911590576, 6.27396821975708, 4.991941452026367, 5.490191459655762, 7.078676700592041, 4.781212329864502, 5.635561466217041, 6.4200615882873535, 6.334233283996582, 5.926899433135986, 4.34764289855957, 2.358856201171875, 4.235803604125977], 'accuracy': [0.2777777910232544, 0.3888888955116272, 0.3888888955116272, 0.3611111044883728, 0.4861111044883728, 0.4722222089767456, 0.5416666865348816, 0.4166666567325592, 0.4583333432674408, 0.4861111044883728, 0.5694444179534912, 0.4583333432674408, 0.5833333134651184, 0.5972222089767456, 0.5694444179534912, 0.4444444477558136, 0.5138888955116272, 0.5694444179534912, 0.6388888955116272, 0.5694444179534912], 'val_loss': [1.3890196084976196, 1.0439534187316895, 1.4302905797958374, 0.9525148868560791, 1.673545002937317, 2.2923033237457275, 1.5752936601638794, 1.6262186765670776, 1.0835351943969727, 2.390636682510376, 5.254139423370361, 1.4425793886184692, 1.8442647457122803, 1.8540905714035034, 1.2069642543792725, 1.199514389038086, 1.627960205078125, 2.650271415710449, 2.979609489440918, 2.8206470012664795], 'val_accuracy': [0.1666666716337204, 0.6666666865348816, 0.3888888955116272, 0.7222222089767456, 0.7222222089767456, 0.1666666716337204, 0.6111111044883728, 0.5555555820465088, 0.6111111044883728, 0.2222222238779068, 0.1666666716337204, 0.5555555820465088, 0.6666666865348816, 0.6666666865348816, 0.5, 0.5, 0.5, 0.2777777910232544, 0.2777777910232544, 0.5], 'lr': [0.001, 0.001, 0.001, 0.001, 0.001, 0.00090000004, 0.00090000004, 0.00090000004, 0.00090000004, 0.00090000004, 0.00081000006, 0.00081000006, 0.00081000006, 0.00081000006, 0.00081000006, 0.0007290001, 0.0007290001, 0.0007290001, 0.0007290001, 0.0007290001]}\n"]}],"source":["import os\n","import cv2\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Dropout\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.callbacks import LearningRateScheduler\n","\n","def scheduler(epoch, lr):\n","    if epoch % 5 == 0 and epoch != 0:\n","        return lr * 0.9\n","    else:\n","        return lr\n","\n","data = []\n","labels = []\n","\n","for folder in os.listdir(\"/content/drive/MyDrive/dataset\"):\n","    for file in os.listdir(os.path.join(\"/content/drive/MyDrive/dataset\", folder)):\n","        img_path = os.path.join(\"/content/drive/MyDrive/dataset\", folder, file)\n","        try:\n","            img = cv2.imread(img_path)\n","            img = cv2.resize(img, (224, 224))\n","            img = img.astype(\"float\") / 255.0\n","            data.append(img)\n","            labels.append(folder)\n","        except cv2.error as e:\n","            print(f\"Error processing image {img_path}: {e}\")\n","\n","data = np.array(data)\n","labels = np.array(labels)\n","\n","(trainX, testX, trainY, testY) = train_test_split(data, labels, test_size=0.2, random_state=42)\n","\n","label_encoder = LabelEncoder()\n","trainY_encoded = label_encoder.fit_transform(trainY)\n","testY_encoded = label_encoder.transform(testY)\n","\n","num_classes = len(label_encoder.classes_)\n","\n","trainY_one_hot = to_categorical(trainY_encoded, num_classes=num_classes)\n","testY_one_hot = to_categorical(testY_encoded, num_classes=num_classes)\n","\n","datagen = ImageDataGenerator(\n","    rotation_range=20,\n","    width_shift_range=0.2,\n","    height_shift_range=0.2,\n","    shear_range=0.2,\n","    zoom_range=0.2,\n","    horizontal_flip=True,\n","    fill_mode='nearest'\n",")\n","\n","datagen.fit(trainX)\n","\n","model = Sequential()\n","model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))\n","model.add(MaxPooling2D(2, 2))\n","model.add(BatchNormalization())\n","model.add(Conv2D(64, (3, 3), activation='relu'))\n","model.add(MaxPooling2D(2, 2))\n","model.add(BatchNormalization())\n","model.add(Conv2D(128, (3, 3), activation='relu'))\n","model.add(MaxPooling2D(2, 2))\n","model.add(BatchNormalization())\n","model.add(Conv2D(256, (3, 3), activation='relu'))\n","model.add(MaxPooling2D(2, 2))\n","model.add(BatchNormalization())\n","model.add(Flatten())\n","model.add(Dense(128, activation='relu'))\n","model.add(Dropout(0.25))\n","model.add(Dense(num_classes, activation='softmax'))\n","\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# Verify data shapes\n","print(\"Shapes - TrainX:\", trainX.shape, \"TrainY:\", trainY_one_hot.shape)\n","print(\"Shapes - TestX:\", testX.shape, \"TestY:\", testY_one_hot.shape)\n","\n","# Verify label encoder classes\n","print(\"Label Encoder Classes:\", label_encoder.classes_)\n","\n","# Model summary\n","model.summary()\n","\n","# Training\n","try:\n","    history = model.fit(datagen.flow(trainX, trainY_one_hot, batch_size=32),\n","                        validation_data=(testX, testY_one_hot),\n","                        epochs=20,\n","                        callbacks=[LearningRateScheduler(scheduler)])\n","except Exception as e:\n","    print(f\"Error during training: {e}\")\n","\n","# Save the model\n","model.save(\"/content/drive/MyDrive/model_multiclass.h5\")\n","\n","# Print history (training and validation loss and accuracy)\n","if 'history' in locals():\n","    print(history.history)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":329459,"status":"ok","timestamp":1703393806920,"user":{"displayName":"PUJARI AISHWARYA","userId":"12213497540452541192"},"user_tz":-330},"id":"AkFOLoqBKbDN","outputId":"fa531cf1-16f0-4b58-f1b1-185ba88c0ee7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training model for class: acne\n","Error processing image /content/drive/MyDrive/dataset/acne/.ipynb_checkpoints: OpenCV(4.8.0) /io/opencv/modules/imgproc/src/resize.cpp:4062: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n","\n","Epoch 1/20\n","2/2 [==============================] - 10s 1s/step - loss: 1.1024 - accuracy: 0.5278 - val_loss: 0.8184 - val_accuracy: 0.0000e+00 - lr: 0.0010\n","Epoch 2/20\n","2/2 [==============================] - 6s 5s/step - loss: 4.7204 - accuracy: 0.7222 - val_loss: 2.0233 - val_accuracy: 0.0000e+00 - lr: 0.0010\n","Epoch 3/20\n","2/2 [==============================] - 6s 5s/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.2519 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 4/20\n","2/2 [==============================] - 7s 1s/step - loss: 4.5751e-07 - accuracy: 1.0000 - val_loss: 0.0113 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 5/20\n","2/2 [==============================] - 7s 1s/step - loss: 6.9280e-05 - accuracy: 1.0000 - val_loss: 4.6441e-04 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 6/20\n","2/2 [==============================] - 5s 850ms/step - loss: 0.9434 - accuracy: 0.9444 - val_loss: 3.0424e-05 - val_accuracy: 1.0000 - lr: 9.0000e-04\n","Epoch 7/20\n","2/2 [==============================] - 7s 6s/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 3.3420e-06 - val_accuracy: 1.0000 - lr: 9.0000e-04\n","Epoch 8/20\n","2/2 [==============================] - 6s 5s/step - loss: 0.3529 - accuracy: 0.9722 - val_loss: 2.6350e-06 - val_accuracy: 1.0000 - lr: 9.0000e-04\n","Epoch 9/20\n","2/2 [==============================] - 7s 6s/step - loss: 0.1348 - accuracy: 0.9722 - val_loss: 1.9129e-05 - val_accuracy: 1.0000 - lr: 9.0000e-04\n","Epoch 10/20\n","2/2 [==============================] - 5s 868ms/step - loss: 5.8949e-15 - accuracy: 1.0000 - val_loss: 1.0827e-04 - val_accuracy: 1.0000 - lr: 9.0000e-04\n","Epoch 11/20\n","2/2 [==============================] - 7s 6s/step - loss: 5.4574e-13 - accuracy: 1.0000 - val_loss: 1.1264e-04 - val_accuracy: 1.0000 - lr: 8.1000e-04\n","Epoch 12/20\n","2/2 [==============================] - 5s 4s/step - loss: 1.6850e-19 - accuracy: 1.0000 - val_loss: 1.1908e-05 - val_accuracy: 1.0000 - lr: 8.1000e-04\n","Epoch 13/20\n","2/2 [==============================] - 7s 6s/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 6.5448e-07 - val_accuracy: 1.0000 - lr: 8.1000e-04\n","Epoch 14/20\n","2/2 [==============================] - 6s 849ms/step - loss: 8.2015e-18 - accuracy: 1.0000 - val_loss: 2.8128e-08 - val_accuracy: 1.0000 - lr: 8.1000e-04\n","Epoch 15/20\n","2/2 [==============================] - 7s 7s/step - loss: 4.8977e-21 - accuracy: 1.0000 - val_loss: 1.1811e-09 - val_accuracy: 1.0000 - lr: 8.1000e-04\n","Epoch 16/20\n","2/2 [==============================] - 6s 863ms/step - loss: 8.2424e-27 - accuracy: 1.0000 - val_loss: 6.9949e-11 - val_accuracy: 1.0000 - lr: 7.2900e-04\n","Epoch 17/20\n","2/2 [==============================] - 5s 821ms/step - loss: 4.0763e-17 - accuracy: 1.0000 - val_loss: 5.1252e-12 - val_accuracy: 1.0000 - lr: 7.2900e-04\n","Epoch 18/20\n","2/2 [==============================] - 10s 2s/step - loss: 6.5103e-08 - accuracy: 1.0000 - val_loss: 3.7820e-13 - val_accuracy: 1.0000 - lr: 7.2900e-04\n","Epoch 19/20\n","2/2 [==============================] - 7s 6s/step - loss: 2.9624e-08 - accuracy: 1.0000 - val_loss: 4.1635e-14 - val_accuracy: 1.0000 - lr: 7.2900e-04\n","Epoch 20/20\n","2/2 [==============================] - 8s 841ms/step - loss: 1.2338 - accuracy: 0.9722 - val_loss: 8.7234e-14 - val_accuracy: 1.0000 - lr: 7.2900e-04\n","Model for class acne trained and saved.\n","Training model for class: acne_scars\n","Epoch 1/20\n","1/1 [==============================] - 5s 5s/step - loss: 1.0957 - accuracy: 0.2000 - val_loss: 0.7073 - val_accuracy: 0.2500 - lr: 0.0010\n","Epoch 2/20\n","1/1 [==============================] - 2s 2s/step - loss: 7.7667e-04 - accuracy: 1.0000 - val_loss: 0.6658 - val_accuracy: 0.7500 - lr: 0.0010\n","Epoch 3/20\n","1/1 [==============================] - 3s 3s/step - loss: 0.5478 - accuracy: 0.9333 - val_loss: 0.6345 - val_accuracy: 0.7500 - lr: 0.0010\n","Epoch 4/20\n","1/1 [==============================] - 4s 4s/step - loss: 0.0818 - accuracy: 0.9333 - val_loss: 0.5739 - val_accuracy: 0.7500 - lr: 0.0010\n","Epoch 5/20\n","1/1 [==============================] - 2s 2s/step - loss: 2.5729 - accuracy: 0.9333 - val_loss: 0.2927 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 6/20\n","1/1 [==============================] - 2s 2s/step - loss: 2.3567e-07 - accuracy: 1.0000 - val_loss: 0.1494 - val_accuracy: 1.0000 - lr: 9.0000e-04\n","Epoch 7/20\n","1/1 [==============================] - 2s 2s/step - loss: 1.7681 - accuracy: 0.9333 - val_loss: 0.0739 - val_accuracy: 1.0000 - lr: 9.0000e-04\n","Epoch 8/20\n","1/1 [==============================] - 2s 2s/step - loss: 0.9194 - accuracy: 0.8667 - val_loss: 0.0447 - val_accuracy: 1.0000 - lr: 9.0000e-04\n","Epoch 9/20\n","1/1 [==============================] - 4s 4s/step - loss: 1.0211e-08 - accuracy: 1.0000 - val_loss: 0.0255 - val_accuracy: 1.0000 - lr: 9.0000e-04\n","Epoch 10/20\n","1/1 [==============================] - 3s 3s/step - loss: 0.9288 - accuracy: 0.9333 - val_loss: 0.0077 - val_accuracy: 1.0000 - lr: 9.0000e-04\n","Epoch 11/20\n","1/1 [==============================] - 2s 2s/step - loss: 3.5558e-16 - accuracy: 1.0000 - val_loss: 0.0024 - val_accuracy: 1.0000 - lr: 8.1000e-04\n","Epoch 12/20\n","1/1 [==============================] - 2s 2s/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 6.2064e-04 - val_accuracy: 1.0000 - lr: 8.1000e-04\n","Epoch 13/20\n","1/1 [==============================] - 3s 3s/step - loss: 0.7708 - accuracy: 0.9333 - val_loss: 6.9917e-05 - val_accuracy: 1.0000 - lr: 8.1000e-04\n","Epoch 14/20\n","1/1 [==============================] - 4s 4s/step - loss: 1.3166e-20 - accuracy: 1.0000 - val_loss: 7.5741e-06 - val_accuracy: 1.0000 - lr: 8.1000e-04\n","Epoch 15/20\n","1/1 [==============================] - 3s 3s/step - loss: 8.8939e-09 - accuracy: 1.0000 - val_loss: 8.0029e-07 - val_accuracy: 1.0000 - lr: 8.1000e-04\n","Epoch 16/20\n","1/1 [==============================] - 2s 2s/step - loss: 6.2105e-15 - accuracy: 1.0000 - val_loss: 9.8920e-08 - val_accuracy: 1.0000 - lr: 7.2900e-04\n","Epoch 17/20\n","1/1 [==============================] - 2s 2s/step - loss: 2.0100e-27 - accuracy: 1.0000 - val_loss: 1.2615e-08 - val_accuracy: 1.0000 - lr: 7.2900e-04\n","Epoch 18/20\n","1/1 [==============================] - 2s 2s/step - loss: 4.2726e-15 - accuracy: 1.0000 - val_loss: 1.5960e-09 - val_accuracy: 1.0000 - lr: 7.2900e-04\n","Epoch 19/20\n","1/1 [==============================] - 4s 4s/step - loss: 9.8367e-15 - accuracy: 1.0000 - val_loss: 1.9534e-10 - val_accuracy: 1.0000 - lr: 7.2900e-04\n","Epoch 20/20\n","1/1 [==============================] - 3s 3s/step - loss: 1.8912e-35 - accuracy: 1.0000 - val_loss: 2.4240e-11 - val_accuracy: 1.0000 - lr: 7.2900e-04\n","Model for class acne_scars trained and saved.\n","Training model for class: hyperPigmentation\n","Epoch 1/20\n","1/1 [==============================] - 5s 5s/step - loss: 1.3171 - accuracy: 0.2667 - val_loss: 0.8809 - val_accuracy: 0.0000e+00 - lr: 0.0010\n","Epoch 2/20\n","1/1 [==============================] - 4s 4s/step - loss: 2.6786 - accuracy: 0.6000 - val_loss: 1.1776 - val_accuracy: 0.0000e+00 - lr: 0.0010\n","Epoch 3/20\n","1/1 [==============================] - 2s 2s/step - loss: 1.0935 - accuracy: 0.8667 - val_loss: 1.2234 - val_accuracy: 0.0000e+00 - lr: 0.0010\n","Epoch 4/20\n","1/1 [==============================] - 2s 2s/step - loss: 1.6436 - accuracy: 0.8667 - val_loss: 0.3418 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 5/20\n","1/1 [==============================] - 2s 2s/step - loss: 0.9067 - accuracy: 0.9333 - val_loss: 0.1531 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 6/20\n","1/1 [==============================] - 2s 2s/step - loss: 1.1121e-07 - accuracy: 1.0000 - val_loss: 0.0784 - val_accuracy: 1.0000 - lr: 9.0000e-04\n","Epoch 7/20\n","1/1 [==============================] - 4s 4s/step - loss: 0.1977 - accuracy: 0.9333 - val_loss: 0.0461 - val_accuracy: 1.0000 - lr: 9.0000e-04\n","Epoch 8/20\n","1/1 [==============================] - 2s 2s/step - loss: 6.0632e-05 - accuracy: 1.0000 - val_loss: 0.0255 - val_accuracy: 1.0000 - lr: 9.0000e-04\n","Epoch 9/20\n","1/1 [==============================] - 2s 2s/step - loss: 9.2758e-10 - accuracy: 1.0000 - val_loss: 0.0134 - val_accuracy: 1.0000 - lr: 9.0000e-04\n","Epoch 10/20\n","1/1 [==============================] - 2s 2s/step - loss: 1.9164e-06 - accuracy: 1.0000 - val_loss: 0.0069 - val_accuracy: 1.0000 - lr: 9.0000e-04\n","Epoch 11/20\n","1/1 [==============================] - 2s 2s/step - loss: 1.0206 - accuracy: 0.9333 - val_loss: 0.0103 - val_accuracy: 1.0000 - lr: 8.1000e-04\n","Epoch 12/20\n","1/1 [==============================] - 4s 4s/step - loss: 1.0179e-16 - accuracy: 1.0000 - val_loss: 0.0211 - val_accuracy: 1.0000 - lr: 8.1000e-04\n","Epoch 13/20\n","1/1 [==============================] - 3s 3s/step - loss: 3.3323e-08 - accuracy: 1.0000 - val_loss: 0.0457 - val_accuracy: 1.0000 - lr: 8.1000e-04\n","Epoch 14/20\n","1/1 [==============================] - 2s 2s/step - loss: 1.0107 - accuracy: 0.9333 - val_loss: 0.0015 - val_accuracy: 1.0000 - lr: 8.1000e-04\n","Epoch 15/20\n","1/1 [==============================] - 2s 2s/step - loss: 2.0427e-14 - accuracy: 1.0000 - val_loss: 6.0899e-05 - val_accuracy: 1.0000 - lr: 8.1000e-04\n","Epoch 16/20\n","1/1 [==============================] - 2s 2s/step - loss: 9.4110e-26 - accuracy: 1.0000 - val_loss: 4.7767e-06 - val_accuracy: 1.0000 - lr: 7.2900e-04\n","Epoch 17/20\n","1/1 [==============================] - 2s 2s/step - loss: 4.9852e-07 - accuracy: 1.0000 - val_loss: 5.8824e-07 - val_accuracy: 1.0000 - lr: 7.2900e-04\n","Epoch 18/20\n","1/1 [==============================] - 3s 3s/step - loss: 2.6932e-26 - accuracy: 1.0000 - val_loss: 9.6732e-08 - val_accuracy: 1.0000 - lr: 7.2900e-04\n","Epoch 19/20\n","1/1 [==============================] - 4s 4s/step - loss: 7.9079e-35 - accuracy: 1.0000 - val_loss: 1.8711e-08 - val_accuracy: 1.0000 - lr: 7.2900e-04\n","Epoch 20/20\n","1/1 [==============================] - 2s 2s/step - loss: 1.5691e-05 - accuracy: 1.0000 - val_loss: 4.2160e-09 - val_accuracy: 1.0000 - lr: 7.2900e-04\n","Model for class hyperPigmentation trained and saved.\n","Training model for class: white_patches\n","Epoch 1/20\n","1/1 [==============================] - 3s 3s/step - loss: 1.1536 - accuracy: 0.6000 - val_loss: 1.0813 - val_accuracy: 0.0000e+00 - lr: 0.0010\n","Epoch 2/20\n","1/1 [==============================] - 1s 779ms/step - loss: 2.0861 - accuracy: 0.8000 - val_loss: 1.0017 - val_accuracy: 0.0000e+00 - lr: 0.0010\n","Epoch 3/20\n","1/1 [==============================] - 1s 775ms/step - loss: 0.2135 - accuracy: 0.8000 - val_loss: 0.6406 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 4/20\n","1/1 [==============================] - 1s 1s/step - loss: 1.6807 - accuracy: 0.8000 - val_loss: 0.4774 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 5/20\n","1/1 [==============================] - 1s 1s/step - loss: 3.2793e-16 - accuracy: 1.0000 - val_loss: 0.3332 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 6/20\n","1/1 [==============================] - 1s 1s/step - loss: 5.0669e-15 - accuracy: 1.0000 - val_loss: 0.2141 - val_accuracy: 1.0000 - lr: 9.0000e-04\n","Epoch 7/20\n","1/1 [==============================] - 2s 2s/step - loss: 3.3170e-20 - accuracy: 1.0000 - val_loss: 0.1301 - val_accuracy: 1.0000 - lr: 9.0000e-04\n","Epoch 8/20\n","1/1 [==============================] - 1s 1s/step - loss: 2.0461e-21 - accuracy: 1.0000 - val_loss: 0.0770 - val_accuracy: 1.0000 - lr: 9.0000e-04\n","Epoch 9/20\n","1/1 [==============================] - 1s 847ms/step - loss: 2.0397e-19 - accuracy: 1.0000 - val_loss: 0.0441 - val_accuracy: 1.0000 - lr: 9.0000e-04\n","Epoch 10/20\n","1/1 [==============================] - 1s 796ms/step - loss: 2.2547 - accuracy: 0.8000 - val_loss: 0.0099 - val_accuracy: 1.0000 - lr: 9.0000e-04\n","Epoch 11/20\n","1/1 [==============================] - 1s 868ms/step - loss: 6.9866e-06 - accuracy: 1.0000 - val_loss: 0.0024 - val_accuracy: 1.0000 - lr: 8.1000e-04\n","Epoch 12/20\n","1/1 [==============================] - 1s 784ms/step - loss: 2.5531e-33 - accuracy: 1.0000 - val_loss: 5.2229e-04 - val_accuracy: 1.0000 - lr: 8.1000e-04\n","Epoch 13/20\n","1/1 [==============================] - 1s 872ms/step - loss: 9.7018e-33 - accuracy: 1.0000 - val_loss: 1.0966e-04 - val_accuracy: 1.0000 - lr: 8.1000e-04\n","Epoch 14/20\n","1/1 [==============================] - 1s 786ms/step - loss: 3.5823e-22 - accuracy: 1.0000 - val_loss: 2.4082e-05 - val_accuracy: 1.0000 - lr: 8.1000e-04\n","Epoch 15/20\n","1/1 [==============================] - 1s 780ms/step - loss: 1.9972e-28 - accuracy: 1.0000 - val_loss: 5.5575e-06 - val_accuracy: 1.0000 - lr: 8.1000e-04\n","Epoch 16/20\n","1/1 [==============================] - 1s 864ms/step - loss: 1.4230e-35 - accuracy: 1.0000 - val_loss: 1.3316e-06 - val_accuracy: 1.0000 - lr: 7.2900e-04\n","Epoch 17/20\n","1/1 [==============================] - 1s 845ms/step - loss: 9.3147e-30 - accuracy: 1.0000 - val_loss: 3.1136e-07 - val_accuracy: 1.0000 - lr: 7.2900e-04\n","Epoch 18/20\n","1/1 [==============================] - 1s 877ms/step - loss: 5.7029e-24 - accuracy: 1.0000 - val_loss: 7.9081e-08 - val_accuracy: 1.0000 - lr: 7.2900e-04\n","Epoch 19/20\n","1/1 [==============================] - 1s 1s/step - loss: 1.2855e-19 - accuracy: 1.0000 - val_loss: 2.1254e-08 - val_accuracy: 1.0000 - lr: 7.2900e-04\n","Epoch 20/20\n","1/1 [==============================] - 1s 1s/step - loss: 9.8226e-35 - accuracy: 1.0000 - val_loss: 5.4443e-09 - val_accuracy: 1.0000 - lr: 7.2900e-04\n","Model for class white_patches trained and saved.\n"]}],"source":["import os\n","import cv2\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Dropout\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.callbacks import LearningRateScheduler\n","\n","def scheduler(epoch, lr):\n","    if epoch % 5 == 0 and epoch != 0:\n","        return lr * 0.9\n","    else:\n","        return lr\n","\n","def build_model():\n","    model = Sequential()\n","    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))\n","    model.add(MaxPooling2D(2, 2))\n","    model.add(BatchNormalization())\n","    model.add(Conv2D(64, (3, 3), activation='relu'))\n","    model.add(MaxPooling2D(2, 2))\n","    model.add(BatchNormalization())\n","    model.add(Conv2D(128, (3, 3), activation='relu'))\n","    model.add(MaxPooling2D(2, 2))\n","    model.add(BatchNormalization())\n","    model.add(Conv2D(256, (3, 3), activation='relu'))\n","    model.add(MaxPooling2D(2, 2))\n","    model.add(BatchNormalization())\n","    model.add(Flatten())\n","    model.add(Dense(128, activation='relu'))\n","    model.add(Dropout(0.25))\n","    model.add(Dense(1, activation='sigmoid'))  # Use 'sigmoid' for binary classification\n","\n","    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","    return model\n","\n","classes = [\"acne\", \"acne_scars\", \"hyperPigmentation\", \"white_patches\"]\n","\n","for class_name in classes:\n","    print(f\"Training model for class: {class_name}\")\n","\n","    # Data Loading and Preprocessing for the specific class\n","    data = []\n","    labels = []\n","\n","    class_path = os.path.join(\"/content/drive/MyDrive/dataset\", class_name)\n","\n","    for file in os.listdir(class_path):\n","        img_path = os.path.join(class_path, file)\n","        try:\n","            img = cv2.imread(img_path)\n","            img = cv2.resize(img, (224, 224))\n","            img = img.astype(\"float\") / 255.0\n","            data.append(img)\n","            labels.append(1)  # Set label to 1 for the specific class, assuming binary classification\n","        except cv2.error as e:\n","            print(f\"Error processing image {img_path}: {e}\")\n","\n","    data = np.array(data)\n","    labels = np.array(labels)\n","\n","    # Split the data into training and testing sets\n","    trainX, testX, trainY, testY = train_test_split(data, labels, test_size=0.2, random_state=42)\n","\n","    # Data Augmentation\n","    datagen = ImageDataGenerator(\n","        rotation_range=20,\n","        width_shift_range=0.2,\n","        height_shift_range=0.2,\n","        shear_range=0.2,\n","        zoom_range=0.2,\n","        horizontal_flip=True,\n","        fill_mode='nearest'\n","    )\n","\n","    datagen.fit(trainX)\n","\n","    # Build and train the model\n","    model = build_model()\n","    model.fit(datagen.flow(trainX, trainY, batch_size=32),\n","              validation_data=(testX, testY),\n","              epochs=20,\n","              callbacks=[LearningRateScheduler(scheduler)])\n","\n","    # Save the model\n","    model.save(f\"/content/drive/MyDrive/model_{class_name}.h5\")\n","\n","    print(f\"Model for class {class_name} trained and saved.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":351474,"status":"ok","timestamp":1703395554359,"user":{"displayName":"PUJARI AISHWARYA","userId":"12213497540452541192"},"user_tz":-330},"id":"ZBTeAODSO65Y","outputId":"43ff3980-b1d2-44bc-9b9e-81816d0f4a40"},"outputs":[{"name":"stdout","output_type":"stream","text":["Processing images for class: acne\n","Error processing image /content/drive/MyDrive/dataset/acne/.ipynb_checkpoints: OpenCV(4.8.0) /io/opencv/modules/imgproc/src/resize.cpp:4062: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n","\n","Processing images for class: acne_scars\n","Processing images for class: hyperPigmentation\n","Processing images for class: white_patches\n","Epoch 1/20\n","3/3 [==============================] - 17s 6s/step - loss: -0.9257 - accuracy: 0.3056 - val_loss: 0.7170 - val_accuracy: 0.6667 - lr: 0.0010\n","Epoch 2/20\n","3/3 [==============================] - 12s 4s/step - loss: -4.7320 - accuracy: 0.2917 - val_loss: 0.6235 - val_accuracy: 0.5556 - lr: 0.0010\n","Epoch 3/20\n","3/3 [==============================] - 13s 3s/step - loss: -20.7570 - accuracy: 0.3611 - val_loss: 0.6222 - val_accuracy: 0.5000 - lr: 0.0010\n","Epoch 4/20\n","3/3 [==============================] - 13s 4s/step - loss: -28.3932 - accuracy: 0.3472 - val_loss: 0.5729 - val_accuracy: 0.1667 - lr: 0.0010\n","Epoch 5/20\n","3/3 [==============================] - 14s 6s/step - loss: -67.4741 - accuracy: 0.3611 - val_loss: 0.6517 - val_accuracy: 0.2778 - lr: 0.0010\n","Epoch 6/20\n","3/3 [==============================] - 12s 3s/step - loss: -88.3908 - accuracy: 0.2778 - val_loss: 1.9211 - val_accuracy: 0.1667 - lr: 9.0000e-04\n","Epoch 7/20\n","3/3 [==============================] - 16s 7s/step - loss: -145.3797 - accuracy: 0.3472 - val_loss: 2.5984 - val_accuracy: 0.2222 - lr: 9.0000e-04\n","Epoch 8/20\n","3/3 [==============================] - 13s 4s/step - loss: -209.5330 - accuracy: 0.3472 - val_loss: 10.9102 - val_accuracy: 0.1111 - lr: 9.0000e-04\n","Epoch 9/20\n","3/3 [==============================] - 12s 5s/step - loss: -289.1297 - accuracy: 0.3194 - val_loss: 8.8870 - val_accuracy: 0.1667 - lr: 9.0000e-04\n","Epoch 10/20\n","3/3 [==============================] - 13s 3s/step - loss: -359.7151 - accuracy: 0.2500 - val_loss: 88.7808 - val_accuracy: 0.1111 - lr: 9.0000e-04\n","Epoch 11/20\n","3/3 [==============================] - 13s 6s/step - loss: -346.4241 - accuracy: 0.2778 - val_loss: 184.0357 - val_accuracy: 0.1111 - lr: 8.1000e-04\n","Epoch 12/20\n","3/3 [==============================] - 10s 3s/step - loss: -520.6117 - accuracy: 0.3333 - val_loss: 107.5410 - val_accuracy: 0.1111 - lr: 8.1000e-04\n","Epoch 13/20\n","3/3 [==============================] - 18s 8s/step - loss: -516.8268 - accuracy: 0.4444 - val_loss: 42.8750 - val_accuracy: 0.1667 - lr: 8.1000e-04\n","Epoch 14/20\n","3/3 [==============================] - 13s 4s/step - loss: -395.9117 - accuracy: 0.2361 - val_loss: 287.4738 - val_accuracy: 0.1111 - lr: 8.1000e-04\n","Epoch 15/20\n","3/3 [==============================] - 14s 4s/step - loss: -585.9633 - accuracy: 0.2778 - val_loss: 322.6111 - val_accuracy: 0.1111 - lr: 8.1000e-04\n","Epoch 16/20\n","3/3 [==============================] - 14s 6s/step - loss: -916.2450 - accuracy: 0.2639 - val_loss: 0.5477 - val_accuracy: 0.5000 - lr: 7.2900e-04\n","Epoch 17/20\n","3/3 [==============================] - 14s 4s/step - loss: -959.1335 - accuracy: 0.2917 - val_loss: 0.5961 - val_accuracy: 0.1111 - lr: 7.2900e-04\n","Epoch 18/20\n","3/3 [==============================] - 13s 3s/step - loss: -968.4462 - accuracy: 0.3611 - val_loss: 0.6022 - val_accuracy: 0.1111 - lr: 7.2900e-04\n","Epoch 19/20\n","3/3 [==============================] - 14s 3s/step - loss: -1257.5533 - accuracy: 0.3056 - val_loss: 464.2912 - val_accuracy: 0.1111 - lr: 7.2900e-04\n","Epoch 20/20\n","3/3 [==============================] - 14s 6s/step - loss: -1248.6420 - accuracy: 0.2500 - val_loss: 543.4930 - val_accuracy: 0.1111 - lr: 7.2900e-04\n","All models trained and saved.\n"]}],"source":["import os\n","import cv2\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Dropout\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.callbacks import LearningRateScheduler\n","\n","def scheduler(epoch, lr):\n","    if epoch % 5 == 0 and epoch != 0:\n","        return lr * 0.9\n","    else:\n","        return lr\n","\n","def build_model():\n","    model = Sequential()\n","    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))\n","    model.add(MaxPooling2D(2, 2))\n","    model.add(BatchNormalization())\n","    model.add(Conv2D(64, (3, 3), activation='relu'))\n","    model.add(MaxPooling2D(2, 2))\n","    model.add(BatchNormalization())\n","    model.add(Conv2D(128, (3, 3), activation='relu'))\n","    model.add(MaxPooling2D(2, 2))\n","    model.add(BatchNormalization())\n","    model.add(Conv2D(256, (3, 3), activation='relu'))\n","    model.add(MaxPooling2D(2, 2))\n","    model.add(BatchNormalization())\n","    model.add(Flatten())\n","    model.add(Dense(128, activation='relu'))\n","    model.add(Dropout(0.25))\n","    model.add(Dense(1, activation='sigmoid'))  # Use 'sigmoid' for binary classification\n","\n","    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","    return model\n","\n","classes = [\"acne\", \"acne_scars\", \"hyperPigmentation\", \"white_patches\"]\n","\n","# Data Loading and Preprocessing for all classes\n","data = []\n","labels = []\n","\n","for class_name in classes:\n","    print(f\"Processing images for class: {class_name}\")\n","\n","    class_path = os.path.join(\"/content/drive/MyDrive/dataset\", class_name)\n","\n","    for file in os.listdir(class_path):\n","        img_path = os.path.join(class_path, file)\n","        try:\n","            img = cv2.imread(img_path)\n","            img = cv2.resize(img, (224, 224))\n","            img = img.astype(\"float\") / 255.0\n","            data.append(img)\n","            labels.append(classes.index(class_name))  # Set label based on class index\n","\n","        except cv2.error as e:\n","            print(f\"Error processing image {img_path}: {e}\")\n","\n","data = np.array(data)\n","labels = np.array(labels)\n","\n","# Split the data into training and testing sets\n","trainX, testX, trainY, testY = train_test_split(data, labels, test_size=0.2, random_state=42)\n","\n","# Data Augmentation\n","datagen = ImageDataGenerator(\n","    rotation_range=20,\n","    width_shift_range=0.2,\n","    height_shift_range=0.2,\n","    shear_range=0.2,\n","    zoom_range=0.2,\n","    horizontal_flip=True,\n","    fill_mode='nearest'\n",")\n","\n","datagen.fit(trainX)\n","\n","# Build and train the model\n","model = build_model()\n","model.fit(datagen.flow(trainX, trainY, batch_size=32),\n","          validation_data=(testX, testY),\n","          epochs=20,\n","          callbacks=[LearningRateScheduler(scheduler)])\n","\n","# Save the model\n","model.save(\"/content/drive/MyDrive/model_all_classes.h5\")\n","\n","print(\"All models trained and saved.\")\n"]},{"cell_type":"code","source":["pip install streamlit\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7Cnc4mp8nm3X","executionInfo":{"status":"ok","timestamp":1704508452126,"user_tz":-330,"elapsed":11539,"user":{"displayName":"PUJARI AISHWARYA","userId":"12213497540452541192"}},"outputId":"51153104-62d7-4e65-f6dc-3ce0563749d3"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting streamlit\n","  Downloading streamlit-1.29.0-py2.py3-none-any.whl (8.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n","Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n","Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.3.2)\n","Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n","Collecting importlib-metadata<7,>=1.4 (from streamlit)\n","  Downloading importlib_metadata-6.11.0-py3-none-any.whl (23 kB)\n","Requirement already satisfied: numpy<2,>=1.19.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.23.5)\n","Requirement already satisfied: packaging<24,>=16.8 in /usr/local/lib/python3.10/dist-packages (from streamlit) (23.2)\n","Requirement already satisfied: pandas<3,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.5.3)\n","Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.4.0)\n","Requirement already satisfied: protobuf<5,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n","Requirement already satisfied: pyarrow>=6.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (10.0.1)\n","Requirement already satisfied: python-dateutil<3,>=2.7.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.8.2)\n","Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.31.0)\n","Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.7.0)\n","Requirement already satisfied: tenacity<9,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.2.3)\n","Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n","Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.5.0)\n","Requirement already satisfied: tzlocal<6,>=1.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.2)\n","Collecting validators<1,>=0.2 (from streamlit)\n","  Downloading validators-0.22.0-py3-none-any.whl (26 kB)\n","Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n","  Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pydeck<1,>=0.8.0b4 (from streamlit)\n","  Downloading pydeck-0.8.1b0-py2.py3-none-any.whl (4.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m57.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.2)\n","Collecting watchdog>=2.1.5 (from streamlit)\n","  Downloading watchdog-3.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.2)\n","Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.19.2)\n","Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.0)\n","Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n","  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<7,>=1.4->streamlit) (3.17.0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2023.3.post1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3,>=2.7.3->streamlit) (1.16.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2023.11.17)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.16.1)\n","Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n","  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.3)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (23.1.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.11.2)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.32.0)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.15.2)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n","Installing collected packages: watchdog, validators, smmap, importlib-metadata, pydeck, gitdb, gitpython, streamlit\n","  Attempting uninstall: importlib-metadata\n","    Found existing installation: importlib-metadata 7.0.0\n","    Uninstalling importlib-metadata-7.0.0:\n","      Successfully uninstalled importlib-metadata-7.0.0\n","Successfully installed gitdb-4.0.11 gitpython-3.1.40 importlib-metadata-6.11.0 pydeck-0.8.1b0 smmap-5.0.1 streamlit-1.29.0 validators-0.22.0 watchdog-3.0.0\n"]}]},{"cell_type":"code","source":["with open('/content/app.py', 'w') as f:\n","    f.write(\"\"\"\n","import streamlit as st\n","import cv2\n","import numpy as np\n","from tensorflow.keras.models import load_model\n","\n","# Load the trained model\n","model = load_model(\"/content/drive/MyDrive/model_multiclass.h5\")\n","\n","# Define class labels\n","class_labels = [\"class1\", \"class2\", \"class3\"]  # Replace with your actual class labels\n","\n","def preprocess_image(image_path):\n","    try:\n","        img = cv2.imread(image_path)\n","        img = cv2.resize(img, (224, 224))\n","        img = img.astype(\"float\") / 255.0\n","        img = np.expand_dims(img, axis=0)\n","        return img\n","    except cv2.error as e:\n","        st.error(f\"Error processing image: {e}\")\n","        return None\n","\n","def make_prediction(image_path):\n","    if image_path:\n","        preprocessed_img = preprocess_image(image_path)\n","        if preprocessed_img is not None:\n","            prediction = model.predict(preprocessed_img)\n","            predicted_class_index = np.argmax(prediction)\n","            predicted_class = class_labels[predicted_class_index]\n","            confidence = prediction[0][predicted_class_index]\n","\n","            st.success(f\"Prediction: {predicted_class} (Confidence: {confidence:.2%})\")\n","            st.image(image_path, caption=\"Uploaded Image\", use_column_width=True)\n","        else:\n","            st.error(\"Failed to process the image.\")\n","    else:\n","        st.warning(\"Please upload an image.\")\n","\n","st.title(\"Image Classification App\")\n","\n","uploaded_file = st.file_uploader(\"Choose an image...\", type=\"jpg\")\n","\n","if st.button(\"Make Prediction\"):\n","    make_prediction(uploaded_file)\n","\"\"\")\n"],"metadata":{"id":"_x1PxQdvnvpw","executionInfo":{"status":"ok","timestamp":1704508737370,"user_tz":-330,"elapsed":6,"user":{"displayName":"PUJARI AISHWARYA","userId":"12213497540452541192"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["!apt-get update\n","!apt-get install -y libgl1-mesa-glx\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y_Fvw_1NtspN","executionInfo":{"status":"ok","timestamp":1704510082815,"user_tz":-330,"elapsed":11455,"user":{"displayName":"PUJARI AISHWARYA","userId":"12213497540452541192"}},"outputId":"e391ab69-ee3a-47cf-d3c9-641443181c31"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n","\r0% [Connecting to archive.ubuntu.com] [Waiting for headers] [1 InRelease 3,626 B/3,626 B 100%] [Conn\r0% [Connecting to archive.ubuntu.com] [Waiting for headers] [Connecting to ppa.launchpadcontent.net \r                                                                                                    \rGet:2 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n","Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n","Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n","Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n","Hit:6 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n","Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n","Get:8 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [1,582 kB]\n","Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n","Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n","Hit:11 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n","Get:12 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,047 kB]\n","Get:13 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,332 kB]\n","Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,307 kB]\n","Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,606 kB]\n","Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [1,611 kB]\n","Fetched 8,718 kB in 3s (3,013 kB/s)\n","Reading package lists... Done\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","The following NEW packages will be installed:\n","  libgl1-mesa-glx\n","0 upgraded, 1 newly installed, 0 to remove and 24 not upgraded.\n","Need to get 5,584 B of archives.\n","After this operation, 74.8 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libgl1-mesa-glx amd64 23.0.4-0ubuntu1~22.04.1 [5,584 B]\n","Fetched 5,584 B in 0s (19.1 kB/s)\n","Selecting previously unselected package libgl1-mesa-glx:amd64.\n","(Reading database ... 121654 files and directories currently installed.)\n","Preparing to unpack .../libgl1-mesa-glx_23.0.4-0ubuntu1~22.04.1_amd64.deb ...\n","Unpacking libgl1-mesa-glx:amd64 (23.0.4-0ubuntu1~22.04.1) ...\n","Setting up libgl1-mesa-glx:amd64 (23.0.4-0ubuntu1~22.04.1) ...\n"]}]},{"cell_type":"code","source":["# Save the Streamlit app code to a file, e.g., app.py\n","app_code = \"\"\"\n","import streamlit as st\n","import cv2\n","import numpy as np\n","import tensorflow as tf\n","\n","# Load the TensorFlow Lite model\n","interpreter = tf.lite.Interpreter(model_path=\"/content/drive/MyDrive/all_model.tflite\")\n","interpreter.allocate_tensors()\n","\n","# Define class labels\n","class_labels = [\"class1\", \"class2\", \"class3\"]  # Replace with your actual class labels\n","\n","def preprocess_image(image_path):\n","    try:\n","        img = cv2.imread(image_path)\n","        img = cv2.resize(img, (224, 224))\n","        img = img.astype(\"float\") / 255.0\n","        img = np.expand_dims(img, axis=0)\n","        return img\n","    except cv2.error as e:\n","        st.error(f\"Error processing image: {e}\")\n","        return None\n","\n","def make_prediction(image_path):\n","    if image_path:\n","        preprocessed_img = preprocess_image(image_path)\n","        if preprocessed_img is not None:\n","            # Perform inference with the TensorFlow Lite model\n","            input_tensor_index = interpreter.get_input_details()[0]['index']\n","            interpreter.set_tensor(input_tensor_index, preprocessed_img)\n","            interpreter.invoke()\n","            output_tensor_index = interpreter.get_output_details()[0]['index']\n","            prediction = interpreter.get_tensor(output_tensor_index)\n","\n","            predicted_class_index = np.argmax(prediction)\n","            predicted_class = class_labels[predicted_class_index]\n","            confidence = prediction[0][predicted_class_index]\n","\n","            st.success(f\"Prediction: {predicted_class} (Confidence: {confidence:.2%})\")\n","            st.image(image_path, caption=\"Uploaded Image\", use_column_width=True)\n","        else:\n","            st.error(\"Failed to process the image.\")\n","    else:\n","        st.warning(\"Please upload an image.\")\n","\n","st.title(\"Image Classification App\")\n","\n","uploaded_file = st.file_uploader(\"Choose an image...\", type=\"jpg\")\n","\n","if st.button(\"Make Prediction\"):\n","    make_prediction(uploaded_file)\n","\"\"\"\n","\n","with open('/content/drive/MyDrive/stream_lit.py', 'w') as f:\n","    f.write(app_code)\n"],"metadata":{"id":"NBlCDy66uFdJ","executionInfo":{"status":"ok","timestamp":1704514667155,"user_tz":-330,"elapsed":506,"user":{"displayName":"PUJARI AISHWARYA","userId":"12213497540452541192"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","\n","# Load your existing model\n","model = tf.keras.models.load_model('/content/drive/MyDrive/model_all_classes.h5')\n","\n","# Convert the model to TensorFlow Lite format with quantization\n","converter = tf.lite.TFLiteConverter.from_keras_model(model)\n","converter.optimizations = [tf.lite.Optimize.DEFAULT]\n","tflite_model = converter.convert()\n","\n","# Save the quantized model\n","with open('quantized_model.tflite', 'wb') as f:\n","    f.write(tflite_model)\n"],"metadata":{"id":"zI786m8V8_1h","executionInfo":{"status":"ok","timestamp":1704514066493,"user_tz":-330,"elapsed":9017,"user":{"displayName":"PUJARI AISHWARYA","userId":"12213497540452541192"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","\n","# Load your existing model\n","model = tf.keras.models.load_model('/content/drive/MyDrive/model_all_classes.h5')\n","\n","# Convert the model to TensorFlow Lite format with quantization\n","converter = tf.lite.TFLiteConverter.from_keras_model(model)\n","converter.optimizations = [tf.lite.Optimize.DEFAULT]\n","tflite_model = converter.convert()\n","\n","# Save the quantized model as all_model.h5\n","with open('/content/drive/MyDrive/all_model.tflite', 'wb') as f:\n","    f.write(tflite_model)\n"],"metadata":{"id":"xw66JalJ9ZxF","executionInfo":{"status":"ok","timestamp":1704514145881,"user_tz":-330,"elapsed":9235,"user":{"displayName":"PUJARI AISHWARYA","userId":"12213497540452541192"}}},"execution_count":9,"outputs":[]}],"metadata":{"colab":{"provenance":[],"mount_file_id":"1X9oif7yayFLl0KEkreJjjQfD54V8vPFq","authorship_tag":"ABX9TyMD5hIXEuE3MVZBW/dDPL2u"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}